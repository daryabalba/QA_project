{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этом блокноте мы реализуем модель вопросно-ответной системы, основанную на подсчёте схожести вопроса и потенциальных ответов."
      ],
      "metadata": {
        "id": "xFq1dMATAQaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data collection"
      ],
      "metadata": {
        "id": "NEgCWsNxPbgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n"
      ],
      "metadata": {
        "id": "shTv26Sbg_Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для бейзлайн-модели нам не нужно как таковое обучение – алгоритм способен работать с текстами сразу – поэтому будем использовать только тестовые данные."
      ],
      "metadata": {
        "id": "t21-pcDRPnDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading val data\n",
        "with open('/content/drive/MyDrive/qasper-dev-v0.3.json', 'r', encoding='utf-8') as file:\n",
        "    qasper_dict = json.load(file)"
      ],
      "metadata": {
        "id": "Xvd-MRPKPad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QimN0iF9298",
        "outputId": "7c6f3a43-88d1-4a66-d614-e3cffba542d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Robustly Leveraging Prior Knowledge in Text Classification',\n",
              " 'abstract': 'Prior knowledge has been shown very useful to address many natural language processing tasks. Many approaches have been proposed to formalise a variety of knowledge, however, whether the proposed approach is robust or sensitive to the knowledge supplied to the model has rarely been discussed. In this paper, we propose three regularization terms on top of generalized expectation criteria, and conduct extensive experiments to justify the robustness of the proposed methods. Experimental results demonstrate that our proposed methods obtain remarkable improvements and are much more robust than baselines.',\n",
              " 'full_text': [{'section_name': 'Introduction',\n",
              "   'paragraphs': ['We posses a wealth of prior knowledge about many natural language processing tasks. For example, in text categorization, we know that words such as NBA, player, and basketball are strong indicators of the sports category BIBREF0 , and words like terrible, boring, and messing indicate a negative polarity while words like perfect, exciting, and moving suggest a positive polarity in sentiment classification.',\n",
              "    'A key problem arisen here, is how to leverage such knowledge to guide the learning process, an interesting problem for both NLP and machine learning communities. Previous studies addressing the problem fall into several lines. First, to leverage prior knowledge to label data BIBREF1 , BIBREF2 . Second, to encode prior knowledge with a prior on parameters, which can be commonly seen in many Bayesian approaches BIBREF3 , BIBREF4 . Third, to formalise prior knowledge with additional variables and dependencies BIBREF5 . Last, to use prior knowledge to control the distributions over latent output variables BIBREF6 , BIBREF7 , BIBREF8 , which makes the output variables easily interpretable.',\n",
              "    \"However, a crucial problem, which has rarely been addressed, is the bias in the prior knowledge that we supply to the learning model. Would the model be robust or sensitive to the prior knowledge? Or, which kind of knowledge is appropriate for the task? Let's see an example: we may be a baseball fan but unfamiliar with hockey so that we can provide a few number of feature words of baseball, but much less of hockey for a baseball-hockey classification task. Such prior knowledge may mislead the model with heavy bias to baseball. If the model cannot handle this situation appropriately, the performance may be undesirable.\",\n",
              "    'In this paper, we investigate into the problem in the framework of Generalized Expectation Criteria BIBREF7 . The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical. To this end, we introduce auxiliary regularization terms in which our prior knowledge is formalized as distribution over output variables. Recall the example just mentioned, though we do not have enough knowledge to provide features for class hockey, it is easy for us to provide some neutral words, namely words that are not strong indicators of any class, like player here. As one of the factors revealed in this paper, supplying neutral feature words can boost the performance remarkably, making the model more robust.',\n",
              "    'More attractively, we do not need manual annotation to label these neutral feature words in our proposed approach.',\n",
              "    'More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. For the first manner, we simply use the most common features as neutral features and assume the neutral features are distributed uniformly over class labels. For the second and third one, we assume we have some knowledge about the class distribution which will be detailed soon later.',\n",
              "    'To summarize, the main contributions of this work are as follows:',\n",
              "    'The rest of the paper is structured as follows: In Section 2, we briefly describe the generalized expectation criteria and present the proposed regularization terms. In Section 3, we conduct extensive experiments to justify the proposed methods. We survey related work in Section 4, and summarize our work in Section 5.']},\n",
              "  {'section_name': 'Method',\n",
              "   'paragraphs': ['We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification.']},\n",
              "  {'section_name': 'Generalized Expectation Criteria',\n",
              "   'paragraphs': ['Generalized expectation (GE) criteria BIBREF7 provides us a natural way to directly constrain the model in the preferred direction. For example, when we know the proportion of each class of the dataset in a classification task, we can guide the model to predict out a pre-specified class distribution.',\n",
              "    \"Formally, in a parameter estimation objective function, a GE term expresses preferences on the value of some constraint functions about the model's expectation. Given a constraint function $G({\\\\rm x}, y)$ , a conditional model distribution $p_\\\\theta (y|\\\\rm x)$ , an empirical distribution $\\\\tilde{p}({\\\\rm x})$ over input samples and a score function $S$ , a GE term can be expressed as follows: \",\n",
              "    '$$S(E_{\\\\tilde{p}({\\\\rm x})}[E_{p_\\\\theta (y|{\\\\rm x})}[G({\\\\rm x}, y)]])$$   (Eq. 4) ']},\n",
              "  {'section_name': 'Learning from Labeled Features',\n",
              "   'paragraphs': ['Druck et al. ge-fl proposed GE-FL to learn from labeled features using generalized expectation criteria. When given a set of labeled features $K$ , the reference distribution over classes of these features is denoted by $\\\\hat{p}(y| x_k), k \\\\in K$ . GE-FL introduces the divergence between this reference distribution and the model predicted distribution $p_\\\\theta (y | x_k)$ , as a term of the objective function: ',\n",
              "    '$$\\\\mathcal {O} = \\\\sum _{k \\\\in K} KL(\\\\hat{p}(y|x_k) || p_\\\\theta (y | x_k)) + \\\\sum _{y,i} \\\\frac{\\\\theta _{yi}^2}{2 \\\\sigma ^2}$$   (Eq. 6) ',\n",
              "    'where $\\\\theta _{yi}$ is the model parameter which indicates the importance of word $i$ to class $y$ . The predicted distribution $p_\\\\theta (y | x_k)$ can be expressed as follows: $\\np_\\\\theta (y | x_k) = \\\\frac{1}{C_k} \\\\sum _{\\\\rm x} p_\\\\theta (y|{\\\\rm x})I(x_k)\\n$ ',\n",
              "    'in which $I(x_k)$ is 1 if feature $k$ occurs in instance ${\\\\rm x}$ and 0 otherwise, $C_k = \\\\sum _{\\\\rm x} I(x_k)$ is the number of instances with a non-zero value of feature $k$ , and $p_\\\\theta (y|{\\\\rm x})$ takes a softmax form as follows: $\\np_\\\\theta (y|{\\\\rm x}) = \\\\frac{1}{Z(\\\\rm x)}\\\\exp (\\\\sum _i \\\\theta _{yi}x_i).\\n$ ',\n",
              "    'To solve the optimization problem, L-BFGS can be used for parameter estimation.',\n",
              "    'In the framework of GE, this term can be obtained by setting the constraint function $G({\\\\rm x}, y) = \\\\frac{1}{C_k} \\\\vec{I} (y)I(x_k)$ , where $\\\\vec{I}(y)$ is an indicator vector with 1 at the index corresponding to label $y$ and 0 elsewhere.']},\n",
              "  {'section_name': 'Regularization Terms',\n",
              "   'paragraphs': ['GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. But as mentioned before, it is often the case that we are not able to provide enough knowledge for some of the classes. For the baseball-hockey classification task, as shown before, GE-FL will predict most of the instances as baseball. In this section, we will show three terms to make the model more robust.',\n",
              "    'Neutral features are features that are not informative indicator of any classes, for instance, word player to the baseball-hockey classification task. Such features are usually frequent words across all categories. When we set the preference distribution of the neutral features to be uniform distributed, these neutral features will prevent the model from biasing to the class that has a dominate number of labeled features.',\n",
              "    'Formally, given a set of neutral features $K^{^{\\\\prime }}$ , the uniform distribution is $\\\\hat{p}_u(y|x_k) = \\\\frac{1}{|C|}, k \\\\in K^{^{\\\\prime }}$ , where $|C|$ is the number of classes. The objective function with the new term becomes ',\n",
              "    '$$\\\\mathcal {O}_{NE} = \\\\mathcal {O} + \\\\sum _{k \\\\in K^{^{\\\\prime }}} KL(\\\\hat{p}_u(y|x_k) || p_\\\\theta (y | x_k)).$$   (Eq. 9) ',\n",
              "    'Note that we do not need manual annotation to provide neutral features. One simple way is to take the most common features as neutral features. Experimental results show that this strategy works successfully.',\n",
              "    'Another way to prevent the model from drifting from the desired direction is to constrain the predicted class distribution on unlabeled data. When lacking knowledge about the class distribution of the data, one feasible way is to take maximum entropy principle, as below: ',\n",
              "    '$$\\\\mathcal {O}_{ME} = \\\\mathcal {O} + \\\\lambda \\\\sum _{y} p(y) \\\\log p(y)$$   (Eq. 11) ',\n",
              "    'where $p(y)$ is the predicted class distribution, given by $\\np(y) = \\\\frac{1}{|X|} \\\\sum _{\\\\rm x} p_\\\\theta (y | \\\\rm x).\\n$ To control the influence of this term on the overall objective function, we can tune $\\\\lambda $ according to the difference in the number of labeled features of each class. In this paper, we simply set $\\\\lambda $ to be proportional to the total number of labeled features, say $\\\\lambda = \\\\beta |K|$ .',\n",
              "    'This maximum entropy term can be derived by setting the constraint function to $G({\\\\rm x}, y) = \\\\vec{I}(y)$ . Therefore, $E_{p_\\\\theta (y|{\\\\rm x})}[G({\\\\rm x}, y)]$ is just the model distribution $p_\\\\theta (y|{\\\\rm x})$ and its expectation with the empirical distribution $\\\\tilde{p}(\\\\rm x)$ is simply the average over input samples, namely $p(y)$ . When $S$ takes the maximum entropy form, we can derive the objective function as above.',\n",
              "    'Sometimes, we have already had much knowledge about the corpus, and can estimate the class distribution roughly without labeling instances. Therefore, we introduce the KL divergence between the predicted and reference class distributions into the objective function.',\n",
              "    'Given the preference class distribution $\\\\hat{p}(y)$ , we modify the objective function as follows: ',\n",
              "    '$$\\\\mathcal {O}_{KL} &= \\\\mathcal {O} + \\\\lambda KL(\\\\hat{p}(y) || p(y))$$   (Eq. 13) ',\n",
              "    'Similarly, we set $\\\\lambda = \\\\beta |K|$ .',\n",
              "    'This divergence term can be derived by setting the constraint function to $G({\\\\rm x}, y) = \\\\vec{I}(y)$ and setting the score function to $S(\\\\hat{p}, p) = \\\\sum _i \\\\hat{p}_i \\\\log \\\\frac{\\\\hat{p}_i}{p_i}$ , where $p$ and $\\\\hat{p}$ are distributions. Note that this regularization term involves the reference class distribution which will be discussed later.']},\n",
              "  {'section_name': 'Experiments',\n",
              "   'paragraphs': ['In this section, we first justify the approach when there exists unbalance in the number of labeled features or in class distribution. Then, to test the influence of $\\\\lambda $ , we conduct some experiments with the method which incorporates the KL divergence of class distribution. Last, we evaluate our approaches in 9 commonly used text classification datasets. We set $\\\\lambda = 5|K|$ by default in all experiments unless there is explicit declaration. The baseline we choose here is GE-FL BIBREF0 , a method based on generalization expectation criteria.']},\n",
              "  {'section_name': 'Data Preparation',\n",
              "   'paragraphs': ['We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features.',\n",
              "    'The movie dataset, in which the task is to classify the movie reviews as positive or negtive, is used for testing the proposed approaches with unbalanced labeled features, unbalanced datasets or different $\\\\lambda $ parameters. All unbalanced datasets are constructed based on the movie dataset by randomly removing documents of the positive class. For each experiment, we conduct 10-fold cross validation.',\n",
              "    'As described in BIBREF0 , there are two ways to obtain labeled features. The first way is to use information gain. We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool. Note that using information gain requires the document label, but this is only to simulate how we human provide prior knowledge to the model. The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).',\n",
              "    'Similar to BIBREF10 , BIBREF0 , we estimate the reference distribution of the labeled features using a heuristic strategy. If there are $|C|$ classes in total, and $n$ classes are associated with a feature $k$ , the probability that feature $k$ is related with any one of the $n$ classes is $\\\\frac{0.9}{n}$ and with any other class is $\\\\frac{0.1}{|C| - n}$ .',\n",
              "    'Neutral features are the most frequent words after removing stop words, and their reference distributions are uniformly distributed. We use the top 10 frequent words as neutral features in all experiments.']},\n",
              "  {'section_name': 'With Unbalanced Labeled Features',\n",
              "   'paragraphs': ['In this section, we evaluate our approach when there is unbalanced knowledge on the categories to be classified. The labeled features are obtained through information gain. Two settings are chosen:',\n",
              "    '(a) We randomly select $t \\\\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. The original balanced movie dataset is used (positive:negative=1:1).',\n",
              "    '(b) Similar to (a), but the dataset is unbalanced, obtained by randomly removing 75% positive documents (positive:negative=1:4).',\n",
              "    'As shown in Figure 1 , Maximum entropy principle shows improvement only on the balanced case. An obvious reason is that maximum entropy only favors uniform distribution.',\n",
              "    'Incorporating Neutral features performs similarly to maximum entropy since we assume that neutral words are uniformly distributed. Its accuracy decreases slowly when the number of labeled features becomes larger ( $t>4$ ) (Figure 1 (a)), suggesting that the model gradually biases to the class with more labeled features, just like GE-FL.',\n",
              "    'Incorporating the KL divergence of class distribution performs much better than GE-FL on both balanced and unbalanced datasets. This shows that it is effective to control the unbalance in labeled features and in the dataset.']},\n",
              "  {'section_name': 'With Balanced Labeled Features',\n",
              "   'paragraphs': ['We also compare with the baseline when the labeled features are balanced. Similar to the experiment above, the labeled features are obtained by information gain. Two settings are experimented with:',\n",
              "    '(a) We randomly select $t \\\\in [1, 20]$ features from the feature pool for each class, and conduct comparisons on the original balanced movie dataset (positive:negtive=1:1).',\n",
              "    '(b) Similar to (a), but the class distribution is unbalanced, by randomly removing 75% positive documents (positive:negative=1:4).',\n",
              "    'Results are shown in Figure 2 . When the dataset is balanced (Figure 2 (a)), there is little difference between GE-FL and our methods. The reason is that the proposed regularization terms provide no additional knowledge to the model and there is no bias in the labeled features. On the unbalanced dataset (Figure 2 (b)), incorporating KL divergence is much better than GE-FL since we provide additional knowledge(the true class distribution), but maximum entropy and neutral features are much worse because forcing the model to approach the uniform distribution misleads it.']},\n",
              "  {'section_name': 'With Unbalanced Class Distributions',\n",
              "   'paragraphs': ['Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. The original balanced movie dataset is used as a control group. We test with both balanced and unbalanced labeled features. For the balanced case, we randomly select 10 features from the feature pool for each class, and for the unbalanced case, we select 10 features for one class, and 1 feature for the other. Results are shown in Figure 3 .',\n",
              "    'Figure 3 (a) shows that when the dataset and the labeled features are both balanced, there is little difference between our methods and GE-FL(also see Figure 2 (a)). But when the class distribution becomes more unbalanced, the difference becomes more remarkable. Performance of neutral features and maximum entropy decrease significantly but incorporating KL divergence increases remarkably. This suggests if we have more accurate knowledge about class distribution, KL divergence can guide the model to the right direction.',\n",
              "    'Figure 3 (b) shows that when the labeled features are unbalanced, our methods significantly outperforms GE-FL. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive.']},\n",
              "  {'section_name': 'The Influence of λ\\\\lambda ',\n",
              "   'paragraphs': ['We present the influence of $\\\\lambda $ on the method that incorporates KL divergence in this section. Since we simply set $\\\\lambda = \\\\beta |K|$ , we just tune $\\\\beta $ here. Note that when $\\\\beta = 0$ , the newly introduced regularization term is disappeared, and thus the model is actually GE-FL. Again, we test the method with different $\\\\lambda $ in two settings:',\n",
              "    '(a) We randomly select $t \\\\in [1, 20]$ features from the feature pool for one class, and only one feature for the other class. The original balanced movie dataset is used (positive:negative=1:1).',\n",
              "    '(b) Similar to (a), but the dataset is unbalanced, obtained by randomly removing 75% positive documents (positive:negative=1:4).',\n",
              "    'Results are shown in Figure 4 . As expected, $\\\\lambda $ reflects how strong the regularization is. The model tends to be closer to our preferences with the increasing of $\\\\lambda $ on both cases.']},\n",
              "  {'section_name': 'Using LDA Selected Features',\n",
              "   'paragraphs': ['We compare our methods with GE-FL on all the 9 datasets in this section. Instead of using features obtained by information gain, we use LDA to select labeled features. Unlike information gain, LDA does not employ any instance labels to find labeled features. In this setting, we can build classification models without any instance annotation, but just with labeled features.',\n",
              "    'Table 1 shows that our three methods significantly outperform GE-FL. Incorporating neutral features performs better than GE-FL on 7 of the 9 datasets, maximum entropy is better on 8 datasets, and KL divergence better on 7 datasets.',\n",
              "    'LDA selects out the most predictive features as labeled features without considering the balance among classes. GE-FL does not exert any control on such an issue, so the performance is severely suffered. Our methods introduce auxiliary regularization terms to control such a bias problem and thus promote the model significantly.']},\n",
              "  {'section_name': 'Related Work',\n",
              "   'paragraphs': ['There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.',\n",
              "    'Liu et al.text manually labeled features which are highly predictive to unsupervised clustering assignments and use them to label unlabeled data. Chang et al.guiding proposed constraint driven learning. They first used constraints and the learned model to annotate unlabeled instances, and then updated the model with the newly labeled data. Daumé daume2008cross proposed a self training method in which several models are trained on the same dataset, and only unlabeled instances that satisfy the cross task knowledge constraints are used in the self training process.',\n",
              "    \"MaCallum et al.gec proposed generalized expectation(GE) criteria which formalised the knowledge as constraint terms about the expectation of the model into the objective function.Graça et al.pr proposed posterior regularization(PR) framework which projects the model's posterior onto a set of distributions that satisfy the auxiliary constraints. Druck et al.ge-fl explored constraints of labeled features in the framework of GE by forcing the model's predicted feature distribution to approach the reference distribution. Andrzejewski et al.andrzejewski2011framework proposed a framework in which general domain knowledge can be easily incorporated into LDA. Altendorf et al.altendorf2012learning explored monotonicity constraints to improve the accuracy while learning from sparse data. Chen et al.chen2013leveraging tried to learn comprehensible topic models by leveraging multi-domain knowledge.\",\n",
              "    'Mann and McCallum simple,generalized incorporated not only labeled features but also other knowledge like class distribution into the objective function of GE-FL. But they discussed only from the semi-supervised perspective and did not investigate into the robustness problem, unlike what we addressed in this paper.',\n",
              "    'There are also some active learning methods trying to use prior knowledge. Raghavan et al.feedback proposed to use feedback on instances and features interlacedly, and demonstrated that feedback on features boosts the model much. Druck et al.active proposed an active learning method which solicits labels on features rather than on instances and then used GE-FL to train the model.']},\n",
              "  {'section_name': 'Conclusion and Discussions',\n",
              "   'paragraphs': ['This paper investigates into the problem of how to leverage prior knowledge robustly in learning models. We propose three regularization terms on top of generalized expectation criteria. As demonstrated by the experimental results, the performance can be considerably improved when taking into account these factors. Comparative results show that our proposed methods is more effective and works more robustly against baselines. To the best of our knowledge, this is the first work to address the robustness problem of leveraging knowledge, and may inspire other research.',\n",
              "    \"We then present more detailed discussions about the three regularization methods. Incorporating neutral features is the simplest way of regularization, which doesn't require any modification of GE-FL but just finding out some common features. But as Figure 1 (a) shows, only using neutral features are not strong enough to handle extremely unbalanced labeled features.\",\n",
              "    'The maximum entropy regularization term shows the strong ability of controlling unbalance.',\n",
              "    \"This method doesn't need any extra knowledge, and is thus suitable when we know nothing about the corpus. But this method assumes that the categories are uniformly distributed, which may not be the case in practice, and it will have a degraded performance if the assumption is violated (see Figure 1 (b), Figure 2 (b), Figure 3 (a)).\",\n",
              "    \"The KL divergence performs much better on unbalanced corpora than other methods. The reason is that KL divergence utilizes the reference class distribution and doesn't make any assumptions. The fact suggests that additional knowledge does benefit the model.\",\n",
              "    'However, the KL divergence term requires providing the true class distribution. Sometimes, we may have the exact knowledge about the true distribution, but sometimes we may not. Fortunately, the model is insensitive to the true distribution and therefore a rough estimation of the true distribution is sufficient. In our experiments, when the true class distribution is 1:2, where the reference class distribution is set to 1:1.5/1:2/1:2.5, the accuracy is 0.755/0.756/0.760 respectively. This provides us the possibility to perform simple computing on the corpus to obtain the distribution in reality. Or, we can set the distribution roughly with domain expertise.']}],\n",
              " 'qas': [{'question': 'What background knowledge do they leverage?',\n",
              "   'question_id': '50be4a737dc0951b35d139f51075011095d77f2a',\n",
              "   'nlp_background': 'infinity',\n",
              "   'topic_background': 'familiar',\n",
              "   'paper_read': 'no',\n",
              "   'search_query': '',\n",
              "   'answers': [{'answer': {'unanswerable': False,\n",
              "      'extractive_spans': ['labeled features'],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': '',\n",
              "      'evidence': ['We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification.'],\n",
              "      'highlighted_evidence': ['We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge.']},\n",
              "     'annotation_id': '72c7d8f0cf398f03592d19718944d0d7eb6c948a',\n",
              "     'worker_id': '18f4d5a2eb93a969d55361267e74aa0c4f6f82fe'},\n",
              "    {'answer': {'unanswerable': False,\n",
              "      'extractive_spans': [],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': 'labelled features, which are words whose presence strongly indicates a specific class or topic',\n",
              "      'evidence': ['We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification.',\n",
              "       'As described in BIBREF0 , there are two ways to obtain labeled features. The first way is to use information gain. We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool. Note that using information gain requires the document label, but this is only to simulate how we human provide prior knowledge to the model. The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).',\n",
              "       'We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features.'],\n",
              "      'highlighted_evidence': ['We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier.',\n",
              "       'As described in BIBREF0 , there are two ways to obtain labeled features.',\n",
              "       'We use bag-of-words feature and remove stopwords in the preprocess stage. ',\n",
              "       'We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool.',\n",
              "       'The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).']},\n",
              "     'annotation_id': 'bd637c246a22f514c938bdfc3eb2509c4d68364e',\n",
              "     'worker_id': '043654eefd60242ac8da08ddc1d4b8d73f86f653'}],\n",
              "   'question_writer': '50d8b4a941c26b89482c94ab324b5a274f9ced66'},\n",
              "  {'question': 'What are the three regularization terms?',\n",
              "   'question_id': '6becff2967fe7c5256fe0b00231765be5b9db9f1',\n",
              "   'nlp_background': 'infinity',\n",
              "   'topic_background': 'familiar',\n",
              "   'paper_read': 'no',\n",
              "   'search_query': '',\n",
              "   'answers': [{'answer': {'unanswerable': False,\n",
              "      'extractive_spans': ['a regularization term associated with neutral features',\n",
              "       'the maximum entropy of class distribution regularization term',\n",
              "       'the KL divergence between reference and predicted class distribution'],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': '',\n",
              "      'evidence': ['More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. For the first manner, we simply use the most common features as neutral features and assume the neutral features are distributed uniformly over class labels. For the second and third one, we assume we have some knowledge about the class distribution which will be detailed soon later.'],\n",
              "      'highlighted_evidence': ['More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.']},\n",
              "     'annotation_id': 'ea5858495aad50966177d146f5e71733ec1def94',\n",
              "     'worker_id': '18f4d5a2eb93a969d55361267e74aa0c4f6f82fe'},\n",
              "    {'answer': {'unanswerable': False,\n",
              "      'extractive_spans': ['a regularization term associated with neutral features',\n",
              "       ' the maximum entropy of class distribution',\n",
              "       'KL divergence between reference and predicted class distribution'],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': '',\n",
              "      'evidence': ['More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. For the first manner, we simply use the most common features as neutral features and assume the neutral features are distributed uniformly over class labels. For the second and third one, we assume we have some knowledge about the class distribution which will be detailed soon later.'],\n",
              "      'highlighted_evidence': ['More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.']},\n",
              "     'annotation_id': 'ec189863dfb5bfb83b9a2ff6f8d1e91a524931ce',\n",
              "     'worker_id': '043654eefd60242ac8da08ddc1d4b8d73f86f653'}],\n",
              "   'question_writer': '50d8b4a941c26b89482c94ab324b5a274f9ced66'},\n",
              "  {'question': 'What NLP tasks do they consider?',\n",
              "   'question_id': '76121e359dfe3f16c2a352bd35f28005f2a40da3',\n",
              "   'nlp_background': 'infinity',\n",
              "   'topic_background': 'familiar',\n",
              "   'paper_read': 'no',\n",
              "   'search_query': '',\n",
              "   'answers': [{'answer': {'unanswerable': False,\n",
              "      'extractive_spans': [],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': 'text classification for themes including sentiment, web-page, science, medical and healthcare',\n",
              "      'evidence': ['In this section, we first justify the approach when there exists unbalance in the number of labeled features or in class distribution. Then, to test the influence of $\\\\lambda $ , we conduct some experiments with the method which incorporates the KL divergence of class distribution. Last, we evaluate our approaches in 9 commonly used text classification datasets. We set $\\\\lambda = 5|K|$ by default in all experiments unless there is explicit declaration. The baseline we choose here is GE-FL BIBREF0 , a method based on generalization expectation criteria.',\n",
              "       'We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features.'],\n",
              "      'highlighted_evidence': [' Last, we evaluate our approaches in 9 commonly used text classification datasets.',\n",
              "       'We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare.']},\n",
              "     'annotation_id': '2c0757cb906ecefa36648ada497fcc9bda53645f',\n",
              "     'worker_id': '043654eefd60242ac8da08ddc1d4b8d73f86f653'}],\n",
              "   'question_writer': '50d8b4a941c26b89482c94ab324b5a274f9ced66'},\n",
              "  {'question': 'How do they define robustness of a model?',\n",
              "   'question_id': '02428a8fec9788f6dc3a86b5d5f3aa679935678d',\n",
              "   'nlp_background': 'infinity',\n",
              "   'topic_background': 'familiar',\n",
              "   'paper_read': 'no',\n",
              "   'search_query': '',\n",
              "   'answers': [{'answer': {'unanswerable': False,\n",
              "      'extractive_spans': [],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': 'ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced',\n",
              "      'evidence': ['GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. But as mentioned before, it is often the case that we are not able to provide enough knowledge for some of the classes. For the baseball-hockey classification task, as shown before, GE-FL will predict most of the instances as baseball. In this section, we will show three terms to make the model more robust.',\n",
              "       '(a) We randomly select $t \\\\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. The original balanced movie dataset is used (positive:negative=1:1).',\n",
              "       'Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. The original balanced movie dataset is used as a control group. We test with both balanced and unbalanced labeled features. For the balanced case, we randomly select 10 features from the feature pool for each class, and for the unbalanced case, we select 10 features for one class, and 1 feature for the other. Results are shown in Figure 3 .',\n",
              "       'Figure 3 (b) shows that when the labeled features are unbalanced, our methods significantly outperforms GE-FL. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive.'],\n",
              "      'highlighted_evidence': ['GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied.',\n",
              "       'We randomly select $t \\\\in [1, 20]$ features from the feature pool for one class, and only one feature for the other.',\n",
              "       'Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents.',\n",
              "       'Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive.']},\n",
              "     'annotation_id': '052bc9840f6733421ea17a3b28753bbec7a871d2',\n",
              "     'worker_id': '043654eefd60242ac8da08ddc1d4b8d73f86f653'},\n",
              "    {'answer': {'unanswerable': False,\n",
              "      'extractive_spans': [],\n",
              "      'yes_no': None,\n",
              "      'free_form_answer': 'Low sensitivity to bias in prior knowledge',\n",
              "      'evidence': [\"However, a crucial problem, which has rarely been addressed, is the bias in the prior knowledge that we supply to the learning model. Would the model be robust or sensitive to the prior knowledge? Or, which kind of knowledge is appropriate for the task? Let's see an example: we may be a baseball fan but unfamiliar with hockey so that we can provide a few number of feature words of baseball, but much less of hockey for a baseball-hockey classification task. Such prior knowledge may mislead the model with heavy bias to baseball. If the model cannot handle this situation appropriately, the performance may be undesirable.\",\n",
              "       'In this paper, we investigate into the problem in the framework of Generalized Expectation Criteria BIBREF7 . The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical. To this end, we introduce auxiliary regularization terms in which our prior knowledge is formalized as distribution over output variables. Recall the example just mentioned, though we do not have enough knowledge to provide features for class hockey, it is easy for us to provide some neutral words, namely words that are not strong indicators of any class, like player here. As one of the factors revealed in this paper, supplying neutral feature words can boost the performance remarkably, making the model more robust.'],\n",
              "      'highlighted_evidence': ['However, a crucial problem, which has rarely been addressed, is the bias in the prior knowledge that we supply to the learning model. Would the model be robust or sensitive to the prior knowledge?',\n",
              "       'The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical.']},\n",
              "     'annotation_id': '95ca58e8d9c9f3445447fc3bcfbd7d902489ddc8',\n",
              "     'worker_id': '18f4d5a2eb93a969d55361267e74aa0c4f6f82fe'}],\n",
              "   'question_writer': '50d8b4a941c26b89482c94ab324b5a274f9ced66'}],\n",
              " 'figures_and_tables': []}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "qasper_dict['1503.00841'] #an example of data structure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "BkWQMMxToBlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6-PGiq2xvre",
        "outputId": "6ea1a888-88ba-4df5-848a-ec4fdd7df5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words_en = stopwords.words('english')\n",
        "stop_words_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4sKn5noxzLu",
        "outputId": "69530dca-0458-4c2c-a9a3-be4b65a47492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# texts preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'[\\w\\']+') # introducing an instance of tokenizer\n",
        "                                              # we treat words with hyphens as two different tokens\n",
        "    lemmatizer = WordNetLemmatizer() # introducing an instance of lemmatizer\n",
        "                                      # our lemmatizer is based on WordNet for English\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(text) # tokenizing texts\n",
        "\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word.lower())\n",
        "                      for word in tokenized_text] # lemmatizing texts (one word at a time)\n",
        "\n",
        "    text_without_stopwords = [word for word in lemmatized_text\n",
        "                              if word not in stop_words_en\n",
        "                              and len(word) > 2] # sorting out stop words\n",
        "\n",
        "    return text_without_stopwords"
      ],
      "metadata": {
        "id": "NHUSWWfvoA2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "BxKMSt3P0Mg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GLdiLTUB8xdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "aX35Lmjdpu5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.info()['models'].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZCP5Z96p0Jm",
        "outputId": "549fd76f-a575-48ef-d22a-c93ac45daf54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель будет работать в два этапа:\n",
        "\n",
        "1.   Из каждого отрывка мы извлечём ключевые слова и сравним среднюю близость каждого из этих слов к словам из вопроса. Таким образом выберем наиболее релевантную для вопроса часть текста.\n",
        "2.   Затем пройдёмся окнами от 1 до n по этому отрывку, сравнивая близость вектора фразы в этом окне с вектором вопроса. Выберем наиболее похожую на вопрос фразу.\n",
        "\n",
        "Недостатки такого подхода очевидны: мы исходим из предположения, что ответ будет похож по смыслу на вопрос или содержит слова из вопроса, что далеко не всегда является правдой.\n",
        "\n"
      ],
      "metadata": {
        "id": "3Myrs-mE0QkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
      ],
      "metadata": {
        "id": "_Ek0n1A6zi7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QASimilarityModel:\n",
        "\n",
        "    def __init__(self, window_size, n_keywords, model):\n",
        "        self.window_size = window_size\n",
        "        self.n_keywords = n_keywords\n",
        "        self.model = model\n",
        "\n",
        "    def _extract_key_words(self, text):\n",
        "        # the idea is to cound words' importance (tfidf value) and take n of them\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "        tfidf_word_features = tfidf_vectorizer.fit_transform([text]).toarray() # counting tfidf values\n",
        "\n",
        "        keywords = sorted(tfidf_vectorizer.vocabulary_,\n",
        "                          key=lambda x:\n",
        "                          tfidf_word_features[0, tfidf_vectorizer.vocabulary_[x]],\n",
        "                          reverse=True) # getting keywords (words with higher tfidf)\n",
        "\n",
        "        thresh = min(self.n_keywords, len(keywords)) # to prevent cases, when n > number of words\n",
        "        return keywords[:thresh]\n",
        "\n",
        "    def _count_mean_similarity(self, keywords, question_words):\n",
        "        simils = []\n",
        "        for q_word in question_words:\n",
        "\n",
        "            q_wordv = self.model[q_word] # getting a vector for a word from the question\n",
        "\n",
        "            simil = []\n",
        "            for k_word in keywords: # counting similarities between keywords and a from the question\n",
        "\n",
        "                try:\n",
        "                    k_wordv = self.model[k_word]\n",
        "                    simil.append(cosine_similarity(q_wordv, k_wordv))\n",
        "                except KeyError:\n",
        "                    simil.append(0.)\n",
        "\n",
        "            simils.append(sum(simil) / len(simil))\n",
        "\n",
        "        return sum(simils) / len(simils)\n",
        "\n",
        "    def _get_relevant_paragraph(self, paragraphs, question_words):\n",
        "        relevance_dict = {}\n",
        "        for parag in paragraphs:\n",
        "\n",
        "            parag_cleaned = ' '.join(preprocess_text(parag)) # getting words from the paragraph\n",
        "            parag_keywords = self._extract_key_words(parag_cleaned) # extracting keywords from the paragraph\n",
        "\n",
        "            # counting the relevance of each paragraph to the question\n",
        "            relevance_dict[parag] = self._count_mean_similarity(parag_keywords, question_words)\n",
        "\n",
        "        # getting the most relevant paragraph\n",
        "        return sorted(relevance_dict, key=lambda x: relevance_dict[x], reverse=True)[0]\n",
        "\n",
        "    def _get_all_spans(self, paragraph):\n",
        "        paragraph = paragraph.split()\n",
        "        spans = []\n",
        "\n",
        "        for n in range(1, self.window_size+1):\n",
        "            spans.extend([paragraph[i:i + n]\n",
        "                          for i in range(len(paragraph))\n",
        "                          if paragraph[i:i + n] not in spans])\n",
        "\n",
        "        return spans\n",
        "\n",
        "    def _get_text_vector(self, text_words):\n",
        "        words_vecs = []\n",
        "        for word in text_words:\n",
        "\n",
        "            try:\n",
        "                words_vecs.append(self.model[word])\n",
        "            except KeyError:\n",
        "                words_vecs.append(np.zeros(300))\n",
        "\n",
        "        return np.mean(np.array(words_vecs))\n",
        "\n",
        "    def get_answer(self, question, text):\n",
        "\n",
        "        question_words = preprocess_text(question) # getting words from the question\n",
        "\n",
        "        paragraphs = []\n",
        "        for parts in text:\n",
        "            paragraphs.extend(parts['paragraphs'])\n",
        "\n",
        "        relevant_paragraph = self._get_relevant_paragraph(paragraphs, question_words)\n",
        "        paragraph_spans = self._get_all_spans(relevant_paragraph)\n",
        "        question_vector = self._get_text_vector(question_words)\n",
        "\n",
        "        spans_relevancy_dict = {}\n",
        "        for span in paragraph_spans:\n",
        "            span_vector = self._get_text_vector(span)\n",
        "            span_similarity = cosine_similarity(question_vector, span_vector)\n",
        "\n",
        "            spans_relevancy_dict[' '.join(span)] = span_similarity\n",
        "\n",
        "        return sorted(spans_relevancy_dict,\n",
        "                      key=lambda x: spans_relevancy_dict[x],\n",
        "                      reverse=True)[0], relevant_paragraph"
      ],
      "metadata": {
        "id": "KSdSFje40Pq3"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model inference"
      ],
      "metadata": {
        "id": "cOfWx0rrtS4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAoTV5LnptnS",
        "outputId": "1a75277b-cc0e-48d4-e460-1a6ff34c7b15"
      },
      "execution_count": 142,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# для оценки будем брать случайные тексты и вопросы к ним из датасета\n",
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R77jW0_BskWn",
        "outputId": "96b7585a-00bb-4548-aa02-b5bb558ecc10"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Improving Question Generation With to the Point Context',\n",
              " 'How big are significant improvements?')"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw3NnadTqKUk",
        "outputId": "2c0de821-92a3-4b38-97db-3b99a2c964c1"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the',\n",
              " 'In the evaluations on the SQuAD dataset, our system achieves significant and consistent improvement as compared to all baseline methods. In particular, we demonstrate that the improvement is more significant with a larger relative distance between the answer and other non-stop sentence words that also appear in the ground truth question. Furthermore, our model is capable of generating diverse questions for a single sentence-answer pair where the sentence conveys multiple relations of its answer fragment.')"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# давайте сравним с реальным ответом (короткий ответ + контекст)\n",
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ6FwEVy7MB6",
        "outputId": "6e77060f-6178-405d-d5b8-00f5ec7fa4c7"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': False,\n",
              " 'extractive_spans': [],\n",
              " 'yes_no': None,\n",
              " 'free_form_answer': 'Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1',\n",
              " 'evidence': ['Table TABREF30 shows automatic evaluation results for our model and baselines (copied from their papers). Our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models BIBREF9, BIBREF15 on both dataset splits. Presumably, our structured answer-relevant relation is a generalization of the context explored by the proximity-based methods because they can only capture short dependencies around answer fragments while our extractions can capture both short and long dependencies given the answer fragments. Moreover, our proposed framework is a general one to jointly leverage structured relations and unstructured sentences. All compared baseline models which only consider unstructured sentences can be further enhanced under our framework.',\n",
              "  'FLOAT SELECTED: Table 4: The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L)'],\n",
              " 'highlighted_evidence': ['Table TABREF30 shows automatic evaluation results for our model and baselines (copied from their papers).',\n",
              "  'FLOAT SELECTED: Table 4: The main experimental results for our model and several baselines. ‘-’ means no results reported in their papers. (Bn: BLEU-n, MET: METEOR, R-L: ROUGE-L)']}"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## сгенерируем ещё несколько примеров"
      ],
      "metadata": {
        "id": "m8-ca8Ql-4Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 пример"
      ],
      "metadata": {
        "id": "J2uhBbKr_Mjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfzDdmSV--N9",
        "outputId": "09accf81-9169-4a3e-f2ff-fff66da35bec"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Neural Collective Entity Linking',\n",
              " 'Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?')"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JmVeCo_Lnc",
        "outputId": "74dbac71-1b26-40ba-d53c-00c485a9e22a"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Local',\n",
              " 'Local features focus on how compatible the entity is mentioned in a piece of text (i.e., the mention and the context words). Except for the prior probability (Section SECREF9 ), we define two types of local features for each candidate entity INLINEFORM0 :')"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu_nbZFC_U5n",
        "outputId": "0b79a361-7502-4137-843f-76982e2a20af"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': False,\n",
              " 'extractive_spans': [],\n",
              " 'yes_no': None,\n",
              " 'free_form_answer': 'NCEL considers only adjacent mentions.',\n",
              " 'evidence': ['Complexity Analysis Compared with local methods, the main disadvantage of collective methods is high complexity and expensive costs. Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence. AIDA has the highest time complexity INLINEFORM3 in worst case due to exhaustive iteratively finding and sorting the graph. The LBP and PageRank/random walk based methods achieve similar high time complexity of INLINEFORM4 mainly because of the inference on the entire graph.'],\n",
              " 'highlighted_evidence': ['Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence.']}"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 пример"
      ],
      "metadata": {
        "id": "a8xs45cs_YiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRkWNgJY_Yxv",
        "outputId": "a5c063fb-b727-44e3-eb0a-92914a3c0ffa"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('A Framework for Evaluation of Machine Reading Comprehension Gold Standards',\n",
              " 'Have they made any attempt to correct MRC gold standards according to their findings? ')"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPetB8zu_Y0t",
        "outputId": "7fada334-eced-4d02-85a2-81921d111e51"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('by',\n",
              " 'One way by which developers of modern crowd-sourced gold standards ensure quality is by having the same entry annotated by multiple workers BIBREF18 and keeping only those with high agreement. We investigate whether this method is enough to establish a sound ground truth answer that is unambiguously correct. Concretely we annotate an answer as Debatable when the passage features multiple plausible answers, when multiple expected answers contradict each other, or an answer is not specific enough with respect to the question and a more specific answer is present. We annotate an answer as Wrong when it is factually wrong and a correct answer is present in the context.')"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaEac_YE_Y5x",
        "outputId": "a2432da1-c86d-457c-ffbc-2b4b56ebafcd"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': False,\n",
              " 'extractive_spans': [],\n",
              " 'yes_no': True,\n",
              " 'free_form_answer': '',\n",
              " 'evidence': ['In this paper, we introduce a novel framework to characterise machine reading comprehension gold standards. This framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach.'],\n",
              " 'highlighted_evidence': ['This framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach.']}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 пример"
      ],
      "metadata": {
        "id": "uxaj-we4AWPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb7-1rScAahF",
        "outputId": "3fa895ac-c451-4513-9f90-d36d93d317bd"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Embedding Geographic Locations for Modelling the Natural Environment using Flickr Tags and Structured Data',\n",
              " 'what dataset is used in this paper?')"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ71-_JQAceM",
        "outputId": "8a99a4ff-1e9b-498c-eb4b-db62c6de10d3"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('is',\n",
              " 'Our work is different from these studies, as our focus is on representing locations based on a given text description of that location (in the form of Flickr tags), along with numerical and categorical features from scientific datasets.')"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf12LCNKAeiQ",
        "outputId": "f1c578f6-3be6-4133-9696-fd4832faa1ff"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': False,\n",
              " 'extractive_spans': [' the same datasets as BIBREF7'],\n",
              " 'yes_no': None,\n",
              " 'free_form_answer': '',\n",
              " 'evidence': ['There is a wide variety of structured data that can be used to describe locations. In this work, we have restricted ourselves to the same datasets as BIBREF7 . These include nine (real-valued) numerical features, which are latitude, longitude, elevation, population, and five climate related features (avg. temperature, avg. precipitation, avg. solar radiation, avg. wind speed, and avg. water vapor pressure). In addition, 180 categorical features were used, which are CORINE land cover classes at level 1 (5 classes), level 2 (15 classes) and level 3 (44 classes) and 116 soil types (SoilGrids). Note that each location should belong to exactly 4 categories: one CORINE class at each of the three levels and a soil type.'],\n",
              " 'highlighted_evidence': [' In this work, we have restricted ourselves to the same datasets as BIBREF7 . These include nine (real-valued) numerical features, which are latitude, longitude, elevation, population, and five climate related features (avg. temperature, avg. precipitation, avg. solar radiation, avg. wind speed, and avg. water vapor pressure). In addition, 180 categorical features were used, which are CORINE land cover classes at level 1 (5 classes), level 2 (15 classes) and level 3 (44 classes) and 116 soil types (SoilGrids). Note that each location should belong to exactly 4 categories: one CORINE class at each of the three levels and a soil type.']}"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 пример"
      ],
      "metadata": {
        "id": "5CbIlCEIAg3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKEp7tKMAgnH",
        "outputId": "8bbf361e-3c5a-4a82-d0b9-699bc46bf505"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Impact of Batch Size on Stopping Active Learning for Text Classification',\n",
              " 'What downstream tasks are evaluated?')"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOvBhlWnAo8f",
        "outputId": "76340b05-b9bb-4f8f-da8e-38c64a7b3fbf"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('An',\n",
              " 'An important aspect of the active learning process is when to stop the active learning process. Stopping methods enable the potential benefits of active learning to be achieved in practice. Without stopping methods, the active learning process would continue until all annotations have been labeled, defeating the purpose of using active learning. Accordingly, there has been a lot of interest in the development of active learning stopping methods BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 .')"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_3Q0tWCAq3m",
        "outputId": "25abd280-4523-4ad3-9b18-05d5d3b09db2"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': True,\n",
              " 'extractive_spans': [],\n",
              " 'yes_no': None,\n",
              " 'free_form_answer': '',\n",
              " 'evidence': [],\n",
              " 'highlighted_evidence': []}"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 пример"
      ],
      "metadata": {
        "id": "La4prseGArhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = qasper_dict[\n",
        "    list(qasper_dict.keys())[np.random.randint(len(qasper_dict))]\n",
        "    ]\n",
        "question_idx = np.random.randint(len(text['qas']))\n",
        "question = text['qas'][question_idx]['question']\n",
        "text['title'], question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtL5Db9RAvtc",
        "outputId": "6560ffe2-024f-40b8-a353-4f597c6d1607"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction',\n",
              " 'How much higher quality is the resulting annotated data?')"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QASimilarityModel(window_size=10, n_keywords=10, model=model)\n",
        "qa_model.get_answer(question, text['full_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZcmX385AxdI",
        "outputId": "9e247b28-6fe8-4e60-e53b-235a0b9a9255"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-538be6afa5f7>:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('We',\n",
              " 'We now examine the possibility that the higher quality and more consistent annotations of domain experts on the difficult instances will benefit the extraction model. This simulates an annotation strategy in which we route difficult instances to domain experts and easier ones to crowd annotators. We also contrast the value of difficult data to that of an i.i.d. random sample of the same size, both annotated by experts.')"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text['qas'][question_idx]['answers'][0]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxEMDSF2A0A5",
        "outputId": "3d29bccd-33cc-4142-c215-b4fdd2291757"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unanswerable': False,\n",
              " 'extractive_spans': ['improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added'],\n",
              " 'yes_no': None,\n",
              " 'free_form_answer': '',\n",
              " 'evidence': ['The results show adding more training data with crowd annotation still improves at least 1 point F1 score in all three extraction tasks. The improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added. The model trained with re-annotating the difficult subset (D+Other) also outperforms the model with re-annotating the random subset (R+Other) by 2 points in F1. The model trained with re-annotating both of difficult and random subsets (D+R+Other), however, achieves only marginally higher F1 than the model trained with the re-annotated difficult subset (D+Other). In sum, the results clearly indicate that mixing expert and crowd annotations leads to better models than using solely crowd data, and better than using expert data alone. More importantly, there is greater gain in performance when instances are routed according to difficulty, as compared to randomly selecting the data for expert annotators. These findings align with our motivating hypothesis that annotation quality for difficult instances is important for final model performance. They also indicate that mixing annotations from expert and crowd could be an effective way to achieve acceptable model performance given a limited budget.'],\n",
              " 'highlighted_evidence': ['The improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added.']}"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По данным примерам делаем вывод, что наша модель способна находить подходящий для вопроса контекст, контекст, в котором может содержаться ответ, однако сам ответ обнаруживается достаточно плохо. Возможно, это как раз связано с тем главным недостатком модели, на который мы указали ранее, также возможно, что нужно использовать другие математические методы помимо вычисления косинусной близости.\n",
        "\n",
        "Что касается получаемых контекстов, они не совпадают с истинными и иногда не содержат ожидаемый нами ответ (то есть ответ, совпадающий с ответом в нашем датасете), однако в этих контекстах часто можно найти в какой-то степени правильный и логичный ответ, пусть иногда слишком расплывчатый и не совсем прямо отвечающий на вопрос (модель напоминает лживого человека, который пытается уйти от ответа на неудобный вопрос). Кажется, что для не model-based подхода наш алгоритм работает в целом неплохо."
      ],
      "metadata": {
        "id": "rFS8SulaB8iH"
      }
    }
  ]
}